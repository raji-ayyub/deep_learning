{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d580169e",
   "metadata": {},
   "source": [
    "## Adam - (Adaptive Moment Estimation) Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c85656c",
   "metadata": {},
   "source": [
    "Adam is one of the most efficient/prominent and widely used machine learning algorithm optimizers for training neural networks.\n",
    "it is classified as a first-order gradient based optimizer and thus uses function evaluation approach (first derivatives).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253d1beb",
   "metadata": {},
   "source": [
    "#### Key features of the Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdbeae9",
   "metadata": {},
   "source": [
    "- **First-Order Gradients:** Adam works by calculating the first derivatives (gradient) of the objective function (which I belive in case of machine learning \"loss function\"), with respect to the model parameters (i.e Xt).\n",
    "This gradient indicates the direction of the steepest ascent.\n",
    "\n",
    "- **Moment Estimation:** The notion \"Adam - Adaptive Moment Estimation\" comes from the fact that its optimizations are moment based.\n",
    "its ability to estimate the average direction and the historical consistency (variability) of the gradients as the model learns. This allows the algorithm to adjust the learning step for each parameter dynamically.\n",
    "\n",
    "- **Efficiency:** Adam provides an efficiency, memory-conscious solution using only first-order information making it less computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e958929",
   "metadata": {},
   "source": [
    "## How the Adam's Algorithm Works (Mathematical Explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94412fa",
   "metadata": {},
   "source": [
    "- **1. Initialization:** Firstly Adam initializes two vectors, m and v, which have the same shape as the parameters ⊖ of the model.\n",
    "\n",
    "    - The vector m to store moving averages of the gradient\n",
    "\n",
    "    - The vector v tracks/stores the moving average of the squared gradients.\n",
    "\n",
    "    They are key to Adam's adaptive adjustments\n",
    "\n",
    "A time stepper t count is also initialized to zero. It keeps track of the number of iterations/updates that the algorithm has completed.\n",
    "\n",
    "The initial values are typically set as follows:\n",
    "\n",
    "- _m_0​=0 (Initial first-moment vector)\n",
    "- _v_0​=0 (Initial second-moment vector)\n",
    "- t=0 (Time step)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
